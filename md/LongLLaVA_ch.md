# LongLLaVA：扩展多模态大语言模型到处理1000张图像的能力

随着人工智能模型的不断发展，**多模态大语言模型 (MLLMs)** 在理解和处理不同类型的数据（如文本、图像和视频）方面取得了显著进步。**LongLLaVA** 是一个前沿的多模态模型，专为处理长上下文的多图像数据（如视频和高分辨率图像）而设计，能够在**单个A100 80GB GPU**上高效地处理多达**1000张图像**。

在这篇博客中，我们将详细探讨**LongLLaVA**的架构、方法以及它在多图像处理任务中的表现。

---

## LongLLaVA 的关键创新点

### 1. 混合架构
![alt text](image.png)

LongLLaVA 的核心创新在于其**混合架构**，该架构结合了 **Mamba 层** 和 **Transformer 层**，以实现对大量视觉数据的高效处理。

- **Mamba 层** 提供了**线性计算复杂度**，使得处理长序列的图像 token 更加高效。
- **Transformer 层** 主要用于需要上下文学习的任务，如理解多张图像之间的关系。

#### a. Transformer 架构
- **计算复杂度**：二次方（Quadratic）。随着输入长度增加，计算复杂度呈平方增长，导致在处理大量图像时效率较低。
- **上下文学习支持**：支持 (✓)。
- **代表模型**：Gemma、LLaMA 等模型。
- **适用场景**：适用于需要强大上下文处理能力的任务，但其计算效率较低，尤其是在大规模图像任务中显得不够高效。
#### b. Mamba 架构
- **计算复杂度**：线性（Linear）。随着输入长度的增加，计算复杂度呈线性增长，使其在处理大量图像时更加高效。
- **上下文学习支持**：不支持 (✗)。
- **代表模型**：Mamba、Mamba-2 等模型。
- **适用场景**：更适合需要高效处理海量数据的场景，尤其在资源有限时表现良好。不过，由于缺乏上下文学习支持，Mamba 在复杂任务中的表现有限。

#### c. Hybrid 混合架构
- **计算复杂度**：准线性（Quasi-Linear），结合了 Transformer 和 Mamba 的优点。
- **上下文学习支持**：支持 (✓)。
- **代表模型**：Jamba、Zamba 等模型。
- **适用场景**：Hybrid 架构在**计算效率**与**上下文学习能力**之间取得了平衡。它可以有效处理长序列图像或视频任务，特别适合需要处理多个图像并且保持高效的应用场景。


通过结合这两种架构，LongLLaVA 在**可扩展性**和**准确性**之间取得了良好的平衡，在处理大量图像任务时表现优异。

### 2. 高效图像表示

为了处理多图像场景，LongLLaVA 采用了 **2D 池化** 来减少输入模型前的图像 token 数量。

- 每张图像最初由 CLIP 编码器生成**576个 tokens**，通过**双线性池化**压缩到**144个 tokens**。
- 这种压缩显著降低了计算负担，使模型能够处理大量图像而不会丢失重要的空间信息。

### 3. 渐进式训练策略
![alt text](image-1.png)

LongLLaVA 通过 **三阶段训练策略** 逐步增强模型处理多图像上下文的能力：

1. **Stage I: 单图像对齐 (Single-image Alignment)**：使用高质量的图像-文本对来对齐视觉和文本模态。
这个阶段的主要目的是将视觉模态特征与文本模态特征对齐。训练使用的数据集包括 **ALLaVA-Caption** 和 **ShareGPT4V**，这些数据集提供了约 **600K** 高质量的图像-文本对。这个阶段仅对 **投影层 (Projector)** 进行训练，而**视觉编码器**和**语言模型 (LLM)** 的参数是冻结的。最终目标是确保模型能够在视觉和文本模态之间建立良好的对齐。
2. **Stage II: 单图像指令微调 (Single-image Instruction-tuning)**：训练模型的多模态指令跟随能力。
在这个阶段，模型通过使用数据集如 **LLaVA-1.5** 和 **Mantis-Single**，总共约 **932K** 高质量的问答对数据进行训练。此时，模型的**视觉编码器**依然是冻结的，主要对**投影层**和**语言模型**进行微调，目的是赋予模型多模态指令跟随的能力。这一过程最终导致了**LongLLaVA** 单图像模型的开发。

3. **Stage III: 多图像指令微调 (Multi-image Instruction-tuning)**：在多图像数据上训练模型，使其能够跨越长视觉上下文进行指令跟随。
第三阶段的训练目标是让模型在多模态长上下文场景中进行指令跟随训练。使用的数据集包括 **Mantis**、**VideoChat2** 和 **ShareGPT4Video**，共采样 **200K**、**200K** 和 **50K** 数据项。此外，还引入了 **Replay** 组件，通过从前一阶段采样的数据项保持模型对单图像理解的能力。这个阶段通过将复杂的单图像划分为多个子图像进行训练，最终开发出了 **LongLLaVA** 的多图像版本。

---



## 视觉信息处理与混合架构

### 1. 视觉信息处理
- LongLLaVA 使用**CLIP**作为视觉编码器，将图像特征编码后通过**两层 MLP** 投射到文本嵌入空间，使其适应语言模型。模型通过**双线性池化**将图像的 token 数量从**576**减少到**144**，通过合并 2x2 的 patch 来减少 token 数量。该策略有效地节省了训练时间并加快了推理速度，同时保持了图像块之间的空间关系。

### 2. 混合大语言模型架构
- LongLLaVA 采用了**混合架构**，其中 **Transformer 层**和 **Mamba 层**以**7:1** 的比例结合使用。这种设计充分利用了 Transformer 的上下文学习能力和 Mamba 的计算效率。
- 模型还引入了 **专家混合 (Mixture of Experts, MoE)** 技术，采用了 16 个专家，并为每个 token 选择其中最优的两个专家。这种方式使模型能够高效处理大规模数据。
- 此外，模型使用 **RMSNorm** 技术进行归一化，并且采用 **Grouped Query Attention (GQA)** 和 **SwiGLU** 激活函数来进一步提升性能。最终，模型在训练时的总参数量达到**530 亿**。

## 数据处理协议

为了确保模型能够有效区分多图像场景中的**时间和空间依赖关系**，并在各种任务中表现良好，**LongLLaVA** 使用了不同的**特殊字符**来处理不同类型的图像输入。以下是具体的处理策略：

### 1. 单图像和多图像处理 (Regular Single and Multiple Images)
- 对于常规的单图像和多图像输入，使用 `<img>` 和 `</img>` 标签将图像 token 包围起来。这有助于模型区分图像 token 和文本 token，确保它们在输入时不会混淆。
- 例如：`<Image>\n What is this?` 表示单个图像和其对应的文本问题。

### 2. 视频处理 (Video Processing)
- 对于视频输入，为了让模型理解帧之间的时间依赖关系，使用 `<vid>` 和 `</vid>` 标签包围图像 token。此外，使用 `<t>` 标签来表示帧之间的时间依赖关系。
- 例如：`<vid><Image><t>...<Image></vid>\n What are they?` 以此表示不同帧之间的时间间隔，从而帮助模型理解视频的时间维度。

### 3. 高分辨率图像处理 (High Resolution Image Processing)
- 对于复杂的单图像处理，尤其是需要将单张图像分成多个子图像的场景，使用 `\n` 来分隔主图像与其子图像。这种方法确保图像的空间关系得以保留。子图像的排列从左上角遍历到右下角，添加分隔符 `\n` 来表示子图像的边界。
- 例如：`<Image>\n<Image>..\n..<Image>\n What are they?` 用于表示多个子图像的组合。

这种**数据处理协议**有效解决了图像之间的各种依赖关系，从而增强了模型在不同任务中的适应性，使 **LongLLaVA** 能够在复杂的多图像和视频理解任务中表现出色。


## 性能与结果

### 基准测试
![alt text](image-2.png)
LongLLaVA 在多个基准测试上表现出色，包括 **MileBench**（长上下文任务）和 **Video-MME**（视频理解）。特别是在**检索**、**排序**和**计数**任务中，LongLLaVA 表现优异。


- **PFLOPs**：代表处理128张图像所需的浮点运算次数，LongLLaVA 的 **PFLOPs** 为 **0.22**，显著低于其他模型，展示了其极高的计算效率。
  
- **MileBench**：LongLLaVA 在 **MileBench** 中表现突出，尤其是在**时间维度**和**信息检索**任务中，得分分别为 **52.7** 和 **67.5**，总平均得分为 **57.4**，在开源模型中位居前列。

- **VideoMME**：在视频理解任务中，特别是**短视频**和**中等视频**长度的测试中，LongLLaVA 的平均得分为 **49.7**，超越了许多视频专用模型如 **Video-LLaMA2**。

- **MVBench**：LongLLaVA 在 **MVBench** 中的得分为 **54.6**，展示了其在多模态任务中的强大处理能力。


- 在 **MileBench** 上，LongLLaVA 超越了其他开源模型，并在某些任务上取得了与**GPT-4V**和**Gemini-1.5-Pro**等闭源模型相当的结果。

### 可扩展性
LongLLaVA 的可扩展性是其一大优势。它能够在单个 A100 GPU 上处理多达 **1000张图像**，且不会显著降低性能。这为以下领域的应用提供了广泛的可能性：

- **视频理解**：LongLLaVA 可以更高效地处理视频帧，提升**事件检测**和**视频摘要**等任务的表现。
- **高分辨率图像分析**：在医学影像或卫星图像分析等需要高分辨率图像处理的领域，LongLLaVA 能够高效处理复杂的视觉输入，辅助**精准诊断**或**地形分析**。

---

## 总结

**LongLLaVA** 代表了多模态 AI 的一次重大进步，提供了一种强大的解决方案，用于大规模视觉数据的处理。通过其混合架构、高效的图像 token 压缩以及渐进式训练策略，LongLLaVA 为多模态长上下文模型设定了新的标准。它不仅在扩展性上表现出色，而且在处理大量图像或视频数据的任务中也表现出色。

这一模型的开发为 **视频理解**、**医学影像** 和 **多模态智能助手** 等领域的 AI 应用指明了未来的方向，在需要处理大量数据集的应用场景中将具有重要影响。

---

**参考文献**：
- [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)
